{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "# from pytorch_toolbelt import losses as L\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# # Albumentations for augmentations\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "# ## using gpu:1\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_Model(nn.Module):\n",
    "    def __init__(self, model_name, cls):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=True)\n",
    "        \n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        # is_rounded, is_backhand, ball_height, is_serve, locationX, locationY\n",
    "        self.fc = nn.Linear(in_features, cls)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.model(image)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform(img_size):\n",
    "    return A.Compose([        \n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(blur_limit= 3, p=0.3),\n",
    "        A.GaussNoise(p=0.3),\n",
    "#         A.OneOf([\n",
    "#             A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
    "#             A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "#         ], p=0.3),        \n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "\n",
    "def get_2s_train_transform(img_size):\n",
    "    return A.Compose([\n",
    "#         A.Resize(img_size, img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_test_transform(img_size):\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Serve_Dataset(Dataset):\n",
    "    def __init__(self, df, img_list, train_img_size, transforms=None):\n",
    "        self.df = df\n",
    "        self.img_list = img_list\n",
    "        self.group = [x.replace('.','')for x in df['group'].tolist()]\n",
    "        self.hitter = df['Hitter'].values\n",
    "        self.hitframe = df['HitFrame'].values\n",
    "        #For label\n",
    "        self.is_serve = [1 if i==1 else 0 for i in df['ShotSeq'].tolist()] \n",
    "        self.hitterx = df['HitterLocationX'].tolist()\n",
    "        self.hittery = df['HitterLocationY'].tolist()\n",
    "        self.train_img_size = train_img_size\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #For Image\n",
    "        hitter_list = ['A','B']\n",
    "        index, hitter_index = index % len(self.df), index // len(self.df)\n",
    "        if hitter_index == 0:\n",
    "            result = [x for x in self.img_list if f'{self.group[index]}_{self.hitter[index]}_hitframe_{self.hitframe[index]}' in x]\n",
    "            is_serve = 1\n",
    "        else:\n",
    "            if self.hitter[index] == 'A':\n",
    "                defender='B'\n",
    "            else:\n",
    "                defender='A'\n",
    "            result = [x for x in self.img_list if f'{self.group[index]}_{defender}_hitframe_{self.hitframe[index]}' in x]\n",
    "            is_serve = 0\n",
    "        path = f'Train_data/mix_sorted_player_bigwidth/all/{result[0]}'\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #For label\n",
    "        x0 = int(result[0].split('_')[4][1:])\n",
    "        y0 = int(result[0].split('_')[5][1:-4])\n",
    "        location_xy = (self.hitterx[index] - x0, self.hittery[index] - y0)\n",
    "        img, location_xy = self.resized_padding(img, self.train_img_size, location_xy)\n",
    "        label_BCE = [is_serve]\n",
    "        label_MSE = [location_xy[0] / self.train_img_size, location_xy[1] / self.train_img_size]\n",
    "        label_MSE = [0 if i < 0 else i for i in label_MSE]\n",
    "        label_MSE = [1 if i > 1 else i for i in label_MSE]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        return {\n",
    "            'image': torch.tensor(img/255, dtype=torch.float32),\n",
    "            'hitter_location': torch.tensor(location_xy, dtype=torch.long),\n",
    "            'label_BCE': torch.tensor(label_BCE, dtype=torch.long),\n",
    "            'label_MSE': torch.tensor(label_MSE, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 2 * len(self.df)\n",
    "    \n",
    "    def resized_padding(self, img, train_img_size, locationXY):\n",
    "        #Resize\n",
    "        h, w = img.shape[:2]\n",
    "        if h >= w:\n",
    "            scale = train_img_size / h\n",
    "            dim = (int(scale * w), train_img_size)\n",
    "        else:\n",
    "            scale = train_img_size / w\n",
    "            dim = (train_img_size, int(scale * h))\n",
    "        resized_locationXY = (int(locationXY[0] * scale), int(locationXY[1] * scale))\n",
    "        resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "        #Padding\n",
    "        left, top = 0, 0\n",
    "        bottom = train_img_size - resized_img.shape[0]\n",
    "        right = train_img_size - resized_img.shape[1]\n",
    "        train_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "        return train_img, resized_locationXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, criterion, optimizer):\n",
    "    scaler= amp.GradScaler()\n",
    "    model.train()\n",
    "\n",
    "    ep_loss_bce = []\n",
    "    for i, data in enumerate(tqdm(dataloader)):\n",
    "        imgs= data['image'].to('cuda')\n",
    "        labels_bce = data['label_BCE'].to('cuda')\n",
    "        \n",
    "        with amp.autocast():\n",
    "            output = model(imgs)\n",
    "            loss_bce = criterion['bce'](output[:,3], labels_bce[:,0].float())\n",
    "            total_loss = loss_bce \n",
    "            ep_loss_bce.append(loss_bce.item())\n",
    "            total_loss/= CFG['gradient_accumulation']\n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "            if (i+1) % CFG['gradient_accumulation']== 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    return np.mean(ep_loss_bce)\n",
    "\n",
    "def valid_epoch(valid_loader, model, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    ep_valid_loss_bce, ep_acc = [], []\n",
    "    preds, labels = [], [] \n",
    "    for i, data in enumerate(tqdm(valid_loader)):\n",
    "        imgs= data['image'].to('cuda')\n",
    "        labels_bce = data['label_BCE'].to('cuda')\n",
    "        hitter_location = data['hitter_location']\n",
    "        output = model(imgs)\n",
    "        loss_bce = criterion['bce'](output[:,3], labels_bce[:,0].float())\n",
    "        valid_loss = loss_bce \n",
    "        ep_valid_loss_bce.append(loss_bce.item())\n",
    "        output = torch.where(output.sigmoid() >= 0.5, 1, 0)\n",
    "        preds.extend([output[i].tolist() for i in range(len(output))])\n",
    "        labels.extend([labels_bce[i].tolist() for i in range(len(labels_bce))])\n",
    "        #output1 : [acc_round,acc_backhandmacc_ball_height,acc,serve] range from 0 - 1\n",
    "    bacc_serve = balanced_accuracy_score([label[0] for label in labels] , [pred[3] for pred in preds])\n",
    "    acc_serve = accuracy_score([label[0] for label in labels] , [pred[3] for pred in preds])\n",
    "    return np.mean(ep_valid_loss_bce), bacc_serve, acc_serve\n",
    "\n",
    "def calculate_acc(output, labels):\n",
    "    #outputs [0.6,0.7,0.3,0.9,.....] -> [1, 1, 0, 1,....] len = batchsize\n",
    "    #labels [1,0,0,1,....] len = batchsize\n",
    "    output[output >= 0.5] = 1\n",
    "    output = output.int()\n",
    "    labels = labels.int()\n",
    "    acc = accuracy_score(labels.tolist(), output.tolist())\n",
    "    recall = recall_score(labels.tolist(), output.tolist())\n",
    "    spe = recall_score(labels.tolist(), output.tolist(), pos_label = 0)\n",
    "    \n",
    "    return (recall + spe) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune model\n"
     ]
    }
   ],
   "source": [
    "CFG= {\n",
    "    'fold': 0,\n",
    "    'epoch': 40,\n",
    "    'model_name': 'tf_efficientnet_b0_ns',\n",
    "    'finetune': True,\n",
    "    \n",
    "    'img_size': 224,\n",
    "    'batch_size': 10,\n",
    "    'gradient_accumulation': 1,\n",
    "    \n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 0,\n",
    "\n",
    "    'num_classes': 4,\n",
    "    'load_model':'',\n",
    "    'save_model': './weight/train'\n",
    "}\n",
    "if CFG['finetune']:\n",
    "    print('finetune model')\n",
    "    CFG['load_model']= f\"weight/cv{CFG['fold']}_effb0_classification_aug_all_best_bigwidth.pth\"\n",
    "    CFG['epoch']= 10\n",
    "    CFG['lr']= 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset(serve)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n",
      "train dataset: 640\n",
      "valid dataset: 160\n",
      "all dataset: 15690\n",
      "load_model: Model/train/cv0_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.47033338434630423\n",
      "valid serve bacc : 0.909375, valid round acc:0.909375\n",
      "all bacc: 0.909375\n",
      "fold : 1\n",
      "train dataset: 640\n",
      "valid dataset: 160\n",
      "all dataset: 15690\n",
      "load_model: Model/train/cv1_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:02<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.5175568693653076\n",
      "valid serve bacc : 0.8875, valid round acc:0.8875\n",
      "all bacc: 0.8875\n",
      "fold : 2\n",
      "train dataset: 640\n",
      "valid dataset: 160\n",
      "all dataset: 15690\n",
      "load_model: Model/train/cv2_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:02<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.3256505203771667\n",
      "valid serve bacc : 0.925, valid round acc:0.925\n",
      "all bacc: 0.925\n",
      "fold : 3\n",
      "train dataset: 640\n",
      "valid dataset: 160\n",
      "all dataset: 15690\n",
      "load_model: Model/train/cv3_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:02<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.297054655609827\n",
      "valid serve bacc : 0.90625, valid round acc:0.90625\n",
      "all bacc: 0.90625\n",
      "fold : 4\n",
      "train dataset: 640\n",
      "valid dataset: 160\n",
      "all dataset: 15690\n",
      "load_model: Model/train/cv4_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.6763166031887522\n",
      "valid serve bacc : 0.903125, valid round acc:0.903125\n",
      "all bacc: 0.903125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img_list = os.listdir('Train_data/mix_sorted_player_bigwidth/all')\n",
    "df= pd.read_csv('Train_data/hitframe.csv')\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f'fold : {fold}')\n",
    "    CFG['load_model'] = f\"weight/cv{fold}_effb0_classification_aug_all_best_bigwidth_serve.pth\"\n",
    "    CFG['fold'] = fold\n",
    "    train_df= df[(df['fold']!=CFG['fold'])&(df['ShotSeq']== 1)]\n",
    "    valid_df= df[(df['fold']==CFG['fold'])&(df['ShotSeq']== 1)]\n",
    "    print(f'train dataset: {len(train_df)}')\n",
    "    print(f'valid dataset: {len(valid_df)}')\n",
    "    print(f'all dataset: {len(img_list)}')\n",
    "\n",
    "    if CFG['finetune']:\n",
    "        train_dataset = Serve_Dataset(train_df, img_list, CFG['img_size'], get_train_transform(CFG['img_size']))\n",
    "    else:\n",
    "        train_dataset = Serve_Dataset(train_df, img_list, CFG['img_size'], get_train_transform(CFG['img_size']))\n",
    "    valid_dataset = Serve_Dataset(valid_df, img_list, CFG['img_size'], get_test_transform(CFG['img_size']))\n",
    "    train_loader = DataLoader(train_dataset, batch_size= CFG['batch_size'], shuffle=True, num_workers=0)\n",
    "    valid_loader= DataLoader(valid_dataset, batch_size=CFG['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    ## create model\n",
    "    if CFG['load_model']:\n",
    "        print(f\"load_model: {CFG['load_model']}\")\n",
    "        model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "    else:\n",
    "        model= Customize_Model(CFG['model_name'], CFG['num_classes'])\n",
    "    model.to('cuda')\n",
    "\n",
    "    ## hyperparameter\n",
    "    criterion = {'bce' : nn.BCEWithLogitsLoss()}\n",
    "    optimizer = optim.AdamW(model.parameters(), lr= CFG['lr'], weight_decay= CFG['weight_decay'])\n",
    "    ## start training\n",
    "    best_score= 0\n",
    "\n",
    "    for ep in range(0, CFG['epoch']+1):\n",
    "        print(f'ep: {ep}')\n",
    "        ## adjust lr\n",
    "        if ep == 50:\n",
    "            model= torch.load(f\"{CFG['save_model']}/cv{CFG['fold']}_effb0_classification_aug_all_best_bigwidth_serve.pth\")\n",
    "            optimizer.param_groups[0]['lr'] = 1e-4\n",
    "            print('Decrease learning rate to 1e-4!')\n",
    "        \n",
    "\n",
    "        writer = SummaryWriter(f\"tensorboard_result/train_cv{CFG['fold']}_effb0_classification_aug_all_best_bigwidth_serve\")\n",
    "        train_loss_bce = train_epoch(train_loader, model, criterion, optimizer)\n",
    "        valid_loss_bce, valid_bacc, valid_acc = valid_epoch(valid_loader, model, criterion)\n",
    "        print(f'train loss bce: {train_loss_bce}')\n",
    "        print(f'valid loss bce: {valid_loss_bce}')\n",
    "        print(f'valid serve bacc : {valid_bacc}, valid round acc:{valid_acc}')\n",
    "        all_bacc = valid_bacc\n",
    "        print(f'all bacc: {all_bacc}')\n",
    "\n",
    "        #writer\n",
    "        writer.add_scalar(\"BCE Loss/train\",train_loss_bce, ep)\n",
    "        writer.add_scalar(\"BCE Loss/val\",valid_loss_bce, ep)\n",
    "        writer.add_scalar(\"All avg recall/val\",all_bacc, ep)\n",
    "\n",
    "        if all_bacc >= best_score:\n",
    "            best_score= all_bacc\n",
    "            torch.save(model, f\"{CFG['save_model']}/cv{CFG['fold']}_effb0_classification_aug_all_best_bigwidth_serve.pth\")\n",
    "            print(f'model save at score: {best_score}')\n",
    "\n",
    "        ## save model every epoch\n",
    "        torch.save(model, f\"{CFG['save_model']}/cv{CFG['fold']}_ep{ep}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test serve acc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def serve_inference(df_serve, model, img_list, train_img_size):\n",
    "    model.eval()\n",
    "    serve_dict = {}\n",
    "    hitframe_list = df_serve['HitFrame'].tolist()\n",
    "    videoname_list = df_serve['group'].tolist()\n",
    "    for i in tqdm(range(len(df_serve))):\n",
    "        hitframe = hitframe_list[i]\n",
    "        videoname = videoname_list[i].replace('.','')\n",
    "        img_path = [x for x in img_list if f'{videoname}_A_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "        img_path_B = [x for x in img_list if f'{videoname}_B_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "        img_path.extend(img_path_B)\n",
    "        result = []\n",
    "        for img_name in img_path:\n",
    "            path = f'sorted_player/mix_sorted_player_bigwidth/all/{img_name}'\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            #For label\n",
    "            img = resized_padding(img, train_img_size).transpose(2,0,1) \n",
    "            img = torch.tensor(img/255, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "            output1 = model(img)\n",
    "            result.append(output1.sigmoid()[0,3])\n",
    "#         print(result)\n",
    "        if result[0] > result[1]:\n",
    "            serve_dict[videoname] = {'server':'A', 'image':img_path[0],'score':float(result[0]),'defender':'B','d_score':float(result[1])}\n",
    "        elif result[0] == result[1]:\n",
    "            print(f'{videoname} : A == B')\n",
    "        else:\n",
    "            serve_dict[videoname] = {'server':'B', 'image':img_path[1],'score':float(result[1]), 'defender':'A','d_score':float(result[0])}\n",
    "    return serve_dict\n",
    "def resized_padding(img, train_img_size):\n",
    "    #Resize\n",
    "    h, w = img.shape[:2]\n",
    "    if h >= w:\n",
    "        scale = train_img_size / h\n",
    "        dim = (int(scale * w), train_img_size)\n",
    "    else:\n",
    "        scale = train_img_size / w\n",
    "        dim = (train_img_size, int(scale * h))\n",
    "    resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "    #Padding\n",
    "    left, top = 0, 0\n",
    "    bottom = train_img_size - resized_img.shape[0]\n",
    "    right = train_img_size - resized_img.shape[1]\n",
    "    train_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    return train_img"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## create model\n",
    "if CFG['load_model']:\n",
    "    print(f\"load_model: {CFG['load_model']}\")\n",
    "    model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "else:\n",
    "    model= Customize_Model(CFG['model_name'], CFG['num_classes'])\n",
    "model.to('cuda')\n",
    "\n",
    "## inference serve\n",
    "img_list = os.listdir('sorted_player/mix_sorted_player_bigwidth/all')\n",
    "df= pd.read_csv('hitframe/hitframe.csv')\n",
    "serve_df= df[(df['fold']==CFG['fold']) & (df['ShotSeq'] == 1)]\n",
    "print(f'serve dataset: {len(serve_df)}')\n",
    "serve_dict = serve_inference(serve_df, model, img_list, CFG['img_size'])\n",
    "\n",
    "##evaluate\n",
    "pred_serve = []\n",
    "true_serve = []\n",
    "for index, row in serve_df.iterrows():\n",
    "    if row['Hitter']=='A':\n",
    "        true_serve.append(0)\n",
    "    else:\n",
    "        true_serve.append(1)\n",
    "    \n",
    "    if serve_dict[row['group'].replace('.','')]['server']=='A':\n",
    "        pred_serve.append(0)\n",
    "    else:\n",
    "        pred_serve.append(1)\n",
    "        \n",
    "    if row['Hitter'] != serve_dict[row['group'].replace('.','')]['server']:\n",
    "        print(row['group'])\n",
    "        print(f\"true : {row['Hitter']}\")\n",
    "        print(f\"pred : {serve_dict[row['group'].replace('.','')]['server']}\")\n",
    "        print(f\"pred score : {serve_dict[row['group'].replace('.','')]['score']}\")\n",
    "        print(f\"defender score : {serve_dict[row['group'].replace('.','')]['d_score']}\")\n",
    "print(f\"serve accuracy : {accuracy_score(true_serve, pred_serve)}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badminton_aicup",
   "language": "python",
   "name": "badminton_aicup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
