{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "# from pytorch_toolbelt import losses as L\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# # Albumentations for augmentations\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "# ## using gpu:1\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_Model(nn.Module):\n",
    "    def __init__(self, model_name, cls):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=True)\n",
    "        \n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        # is_rounded, is_backhand, ball_height, is_serve, locationX, locationY\n",
    "        self.fc = nn.Linear(in_features, cls)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.model(image)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform(img_size):\n",
    "    return A.Compose([        \n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(blur_limit= 3, p=0.3),\n",
    "        A.GaussNoise(p=0.3),\n",
    "#         A.OneOf([\n",
    "#             A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
    "#             A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "#         ], p=0.3),        \n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "\n",
    "def get_2s_train_transform(img_size):\n",
    "    return A.Compose([\n",
    "#         A.Resize(img_size, img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_test_transform(img_size):\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_Dataset(Dataset):\n",
    "    def __init__(self, df, img_list, train_img_size, transforms=None):\n",
    "        self.df = df\n",
    "        self.img_list = img_list\n",
    "        self.group = [x.replace('.','')for x in df['group'].tolist()]\n",
    "        self.hitter = df['Hitter'].values\n",
    "        self.hitframe = df['HitFrame'].values\n",
    "        #For label\n",
    "        self.is_roundhead = [0 if i==2 else i for i in df['RoundHead'].tolist()]\n",
    "        self.is_backhand = [0 if i==2 else i for i in df['Backhand'].tolist()] \n",
    "        self.is_serve = [1 if i==1 else 0 for i in df['ShotSeq'].tolist()] \n",
    "        self.ball_height = [0 if i==2 else i for i in df['BallHeight'].tolist()] \n",
    "        self.hitterx = df['HitterLocationX'].tolist()\n",
    "        self.hittery = df['HitterLocationY'].tolist()\n",
    "        self.train_img_size = train_img_size\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #For Image\n",
    "        result = [x for x in self.img_list if f'{self.group[index]}_{self.hitter[index]}_hitframe_{self.hitframe[index]}' in x]\n",
    "        path = f'./Train_data/sorted_player_bigwidth/hitter/{result[0]}'\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #For label\n",
    "        x0 = int(result[0].split('_')[4][1:])\n",
    "        y0 = int(result[0].split('_')[5][1:-4])\n",
    "        location_xy = (self.hitterx[index] - x0, self.hittery[index] - y0)\n",
    "        img, location_xy = self.resized_padding(img, self.train_img_size, location_xy)\n",
    "        label_BCE = [self.is_roundhead[index], self.is_backhand[index], self.ball_height[index], self.is_serve[index]]\n",
    "        label_MSE = [location_xy[0] / self.train_img_size, location_xy[1] / self.train_img_size]\n",
    "        label_MSE = [0 if i < 0 else i for i in label_MSE]\n",
    "        label_MSE = [1 if i > 1 else i for i in label_MSE]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        return {\n",
    "            'image': torch.tensor(img/255, dtype=torch.float32),\n",
    "            'hitter_location': torch.tensor(location_xy, dtype=torch.long),\n",
    "            'label_BCE': torch.tensor(label_BCE, dtype=torch.long),\n",
    "            'label_MSE': torch.tensor(label_MSE, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def resized_padding(self, img, train_img_size, locationXY):\n",
    "        #Resize\n",
    "        h, w = img.shape[:2]\n",
    "        if h >= w:\n",
    "            scale = train_img_size / h\n",
    "            dim = (int(scale * w), train_img_size)\n",
    "        else:\n",
    "            scale = train_img_size / w\n",
    "            dim = (train_img_size, int(scale * h))\n",
    "        resized_locationXY = (int(locationXY[0] * scale), int(locationXY[1] * scale))\n",
    "        resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "        #Padding\n",
    "        left, top = 0, 0\n",
    "        bottom = train_img_size - resized_img.shape[0]\n",
    "        right = train_img_size - resized_img.shape[1]\n",
    "        train_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "        return train_img, resized_locationXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, criterion, optimizer):\n",
    "    scaler= amp.GradScaler()\n",
    "    model.train()\n",
    "\n",
    "    ep_loss_bce = []\n",
    "    for i, data in enumerate(tqdm(dataloader)):\n",
    "        imgs= data['image'].to('cuda')\n",
    "        labels_bce = data['label_BCE'].to('cuda')\n",
    "        \n",
    "        with amp.autocast():\n",
    "            output = model(imgs)\n",
    "            loss_bce = criterion['bce'](output, labels_bce.float())\n",
    "            total_loss = loss_bce \n",
    "            ep_loss_bce.append(loss_bce.item())\n",
    "            total_loss/= CFG['gradient_accumulation']\n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "            if (i+1) % CFG['gradient_accumulation']== 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    return np.mean(ep_loss_bce)\n",
    "\n",
    "def valid_epoch(valid_loader, model, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    ep_valid_loss_bce, ep_acc = [], []\n",
    "    preds, labels = [], [] \n",
    "    for i, data in enumerate(tqdm(valid_loader)):\n",
    "        imgs= data['image'].to('cuda')\n",
    "        labels_bce = data['label_BCE'].to('cuda')\n",
    "        hitter_location = data['hitter_location']\n",
    "        output = model(imgs)\n",
    "        loss_bce = criterion['bce'](output, labels_bce.float())\n",
    "        valid_loss = loss_bce \n",
    "        ep_valid_loss_bce.append(loss_bce.item())\n",
    "        output = torch.where(output.sigmoid() >= 0.5, 1, 0)\n",
    "        preds.extend([output[i].tolist() for i in range(len(output))])\n",
    "        labels.extend([labels_bce[i].tolist() for i in range(len(labels_bce))])\n",
    "        #output1 : [acc_round,acc_backhandmacc_ball_height,acc,serve] range from 0 - 1\n",
    "    bacc_round = balanced_accuracy_score([label[0] for label in labels] , [pred[0] for pred in preds])\n",
    "    bacc_backhand = balanced_accuracy_score([label[1] for label in labels] , [pred[1] for pred in preds])\n",
    "    bacc_ballheight = balanced_accuracy_score([label[2] for label in labels] , [pred[2] for pred in preds])\n",
    "    bacc_serve = balanced_accuracy_score([label[3] for label in labels] , [pred[3] for pred in preds])\n",
    "    ep_bacc = [bacc_round, bacc_backhand, bacc_ballheight, bacc_serve]\n",
    "    \n",
    "    acc_round = accuracy_score([label[0] for label in labels] , [pred[0] for pred in preds])\n",
    "    acc_backhand = accuracy_score([label[1] for label in labels] , [pred[1] for pred in preds])\n",
    "    acc_ballheight = accuracy_score([label[2] for label in labels] , [pred[2] for pred in preds])\n",
    "    acc_serve = accuracy_score([label[3] for label in labels] , [pred[3] for pred in preds])\n",
    "    ep_acc = [acc_round, acc_backhand, acc_ballheight, acc_serve]\n",
    "    return np.mean(ep_valid_loss_bce), ep_bacc, ep_acc\n",
    "\n",
    "def calculate_acc(output, labels):\n",
    "    #outputs [0.6,0.7,0.3,0.9,.....] -> [1, 1, 0, 1,....] len = batchsize\n",
    "    #labels [1,0,0,1,....] len = batchsize\n",
    "    output[output >= 0.5] = 1\n",
    "    output = output.int()\n",
    "    labels = labels.int()\n",
    "    acc = accuracy_score(labels.tolist(), output.tolist())\n",
    "    recall = recall_score(labels.tolist(), output.tolist())\n",
    "    spe = recall_score(labels.tolist(), output.tolist(), pos_label = 0)\n",
    "    \n",
    "    return (recall + spe) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'fold': 0,\n",
    "    'epoch': 40,\n",
    "    'model_name': 'tf_efficientnet_b0_ns',\n",
    "    'finetune': False,\n",
    "    \n",
    "    'img_size': 224,\n",
    "    'batch_size': 10,\n",
    "    'gradient_accumulation': 1,\n",
    "    \n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 0,\n",
    "\n",
    "    'num_classes': 4,\n",
    "    'load_model':'',\n",
    "    'save_model': './weight/train'\n",
    "}\n",
    "if CFG['finetune']:\n",
    "    print('finetune model')\n",
    "    CFG['load_model']= f\"weight/cv{CFG['fold']}_effb0_classification_best_bigwidth.pth\"\n",
    "    CFG['epoch']= 5\n",
    "    CFG['lr']= 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 6276\n",
      "valid dataset: 1569\n",
      "all dataset: 7845\n",
      "fold : 0\n",
      "load_model: ./Model/train/cv0_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 157/157 [00:09<00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.19899214171673985\n",
      "valid round bacc: 0.9418662152652301, valid round acc:0.9604843849585724\n",
      "valid backhand bacc: 0.9475882594417078, valid round acc:0.9509241555130656\n",
      "valid ballheight bacc: 0.946125843557738, valid round acc:0.9502868068833652\n",
      "valid serve bacc : 0.9336652767920511, valid round acc:0.9853409815168898\n",
      "all bacc: 0.9423113987641818\n",
      "train dataset: 6276\n",
      "valid dataset: 1569\n",
      "all dataset: 7845\n",
      "fold : 1\n",
      "load_model: ./Model/train/cv1_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 157/157 [00:10<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.21893812787265274\n",
      "valid round bacc: 0.9243217054263566, valid round acc:0.9388145315487572\n",
      "valid backhand bacc: 0.9366959199859913, valid round acc:0.9420012746972594\n",
      "valid ballheight bacc: 0.9389185331263972, valid round acc:0.9369024856596558\n",
      "valid serve bacc : 0.921875, valid round acc:0.9840662842574889\n",
      "all bacc: 0.9304527896346864\n",
      "train dataset: 6276\n",
      "valid dataset: 1569\n",
      "all dataset: 7845\n",
      "fold : 2\n",
      "load_model: ./Model/train/cv2_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 157/157 [00:10<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.2548962981354113\n",
      "valid round bacc: 0.9026924005497159, valid round acc:0.935627788400255\n",
      "valid backhand bacc: 0.9378042208469508, valid round acc:0.9407265774378585\n",
      "valid ballheight bacc: 0.9387796343277054, valid round acc:0.9451880178457617\n",
      "valid serve bacc : 0.95618124556423, valid round acc:0.9859783301465902\n",
      "all bacc: 0.9338643753221505\n",
      "train dataset: 6276\n",
      "valid dataset: 1569\n",
      "all dataset: 7845\n",
      "fold : 3\n",
      "load_model: ./Model/train/cv3_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 157/157 [00:10<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.1507657235162036\n",
      "valid round bacc: 0.9402029391182645, valid round acc:0.9732313575525813\n",
      "valid backhand bacc: 0.9456119135405743, valid round acc:0.9490121096239643\n",
      "valid ballheight bacc: 0.9396217637836808, valid round acc:0.9502868068833652\n",
      "valid serve bacc : 0.9465201383960256, valid round acc:0.988527724665392\n",
      "all bacc: 0.9429891887096362\n",
      "train dataset: 6276\n",
      "valid dataset: 1569\n",
      "all dataset: 7845\n",
      "fold : 4\n",
      "load_model: ./Model/train/cv4_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 157/157 [00:10<00:00, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss bce: 0.27332440516904033\n",
      "valid round bacc: 0.9256882938176417, valid round acc:0.9623964308476737\n",
      "valid backhand bacc: 0.9511530625368052, valid round acc:0.9553855959209687\n",
      "valid ballheight bacc: 0.9370252535295287, valid round acc:0.9502868068833652\n",
      "valid serve bacc : 0.9357256919801278, valid round acc:0.9840662842574889\n",
      "all bacc: 0.9373980754660258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img_list = os.listdir('Train_data/sorted_player_bigwidth/hitter')\n",
    "df= pd.read_csv('Train_data/hitframe.csv')\n",
    "\n",
    "for fold in range(5):\n",
    "    CFG['fold'] = fold\n",
    "    train_df= df[df['fold']!=CFG['fold']]\n",
    "    valid_df= df[df['fold']==CFG['fold']]\n",
    "    print(f'train dataset: {len(train_df)}')\n",
    "    print(f'valid dataset: {len(valid_df)}')\n",
    "    print(f'all dataset: {len(img_list)}')\n",
    "    print(f'fold : {CFG[\"fold\"]}')\n",
    "\n",
    "    if CFG['finetune']:\n",
    "        train_dataset = Customize_Dataset(train_df, get_2s_train_transform(CFG['img_size']))\n",
    "    else:\n",
    "        train_dataset = Customize_Dataset(train_df, img_list, CFG['img_size'], get_train_transform(CFG['img_size']))\n",
    "    valid_dataset = Customize_Dataset(valid_df, img_list, CFG['img_size'], get_test_transform(CFG['img_size']))\n",
    "    train_loader = DataLoader(train_dataset, batch_size= CFG['batch_size'], shuffle=True, num_workers=0)\n",
    "    valid_loader= DataLoader(valid_dataset, batch_size=CFG['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    ## create model\n",
    "    if CFG['load_model']:\n",
    "        print(f'load_model: {CFG[\"load_model\"]}')\n",
    "        model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "    else:\n",
    "        model= Customize_Model(CFG['model_name'], CFG['num_classes'])\n",
    "    model.to('cuda')\n",
    "\n",
    "    ## hyperparameter\n",
    "    criterion = {'bce' : nn.BCEWithLogitsLoss()}\n",
    "    optimizer = optim.AdamW(model.parameters(), lr= CFG['lr'], weight_decay= CFG['weight_decay'])\n",
    "    ## start training\n",
    "    best_score= 0 \n",
    "\n",
    "    for ep in range(0, CFG['epoch']+1):\n",
    "        print(f'ep: {ep}')\n",
    "        ## adjust lr\n",
    "        if ep == 50:\n",
    "            model= torch.load(f\"{CFG['save_model']}/cv{CFG['fold']}_effb0_classification_aug_all_best_bigwidth.pth\")\n",
    "            optimizer.param_groups[0]['lr'] = 1e-4\n",
    "            print('Decrease learning rate to 1e-4!')\n",
    "\n",
    "        writer = SummaryWriter(f\"tensorboard_result/train_cv{CFG['fold']}_effb0_classification_aug_all_best_bigwidth\")\n",
    "        train_loss_bce = train_epoch(train_loader, model, criterion, optimizer)\n",
    "        valid_loss_bce, valid_bacc, valid_acc = valid_epoch(valid_loader, model, criterion)\n",
    "        print(f'train loss bce: {train_loss_bce}')\n",
    "        print(f'valid loss bce: {valid_loss_bce}')\n",
    "        print(f'valid round bacc: {valid_bacc[0]}, valid round acc:{valid_acc[0]}')\n",
    "        print(f'valid backhand bacc: {valid_bacc[1]}, valid round acc:{valid_acc[1]}')\n",
    "        print(f'valid ballheight bacc: {valid_bacc[2]}, valid round acc:{valid_acc[2]}')\n",
    "        print(f'valid serve bacc : {valid_bacc[3]}, valid round acc:{valid_acc[3]}')\n",
    "        all_bacc = np.mean(valid_bacc)\n",
    "        print(f'all bacc: {all_bacc}')\n",
    "\n",
    "        #writer\n",
    "        writer.add_scalar(\"BCE Loss/train\",train_loss_bce, ep)\n",
    "        writer.add_scalar(\"BCE Loss/val\",valid_loss_bce, ep)\n",
    "        writer.add_scalar(\"Round avg recall/val\",valid_bacc[0], ep)\n",
    "        writer.add_scalar(\"Backhand avg recall/val\",valid_bacc[1], ep)\n",
    "        writer.add_scalar(\"Ballheight avg recall/val\",valid_bacc[2], ep)\n",
    "        writer.add_scalar(\"Serve avg recall/val\",valid_bacc[3], ep)\n",
    "        writer.add_scalar(\"All avg recall/val\",all_bacc, ep)\n",
    "\n",
    "        if all_bacc >= best_score:\n",
    "            best_score= all_bacc\n",
    "            torch.save(model, f\"{CFG['save_model']}/cv{CFG['fold']}_effb0_classification_aug_all_best_bigwidth.pth\")\n",
    "            print(f'model save at score: {best_score}')\n",
    "\n",
    "        ## save model every epoch\n",
    "        torch.save(model, f\"{CFG['save_model']}/cv{CFG['fold']}_ep{ep}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## create model\n",
    "if CFG['load_model']:\n",
    "    print(f\"load_model: {CFG['load_model']}\")\n",
    "    model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "else:\n",
    "    model= Customize_Model(CFG['model_name'], CFG['num_classes'])\n",
    "model.to('cuda')\n",
    "    \n",
    "## hyperparameter\n",
    "criterion = {'bce' : nn.BCEWithLogitsLoss()}\n",
    "optimizer = optim.AdamW(model.parameters(), lr= CFG['lr'], weight_decay= CFG['weight_decay'])\n",
    "## start training\n",
    "best_score= 0\n",
    "\n",
    "for ep in range(0, CFG['epoch']+1):\n",
    "    print(f'ep: {ep}')\n",
    "    \n",
    "    ## adjust lr\n",
    "    if ep == 50:\n",
    "        model= torch.load(f\"{CFG['save_model']}/cv{CFG['fold']}_effb0_classification_best_bigwidth_serve.pth\")\n",
    "        optimizer.param_groups[0]['lr'] = 1e-4\n",
    "        print('Decrease learning rate to 1e-4!')\n",
    "    \n",
    "    writer = SummaryWriter(f\"tensorboard_result/train_cv{CFG['fold']}_effb0_classification_best_bigwidth_serve\")\n",
    "    train_loss_bce = train_epoch(train_loader, model, criterion, optimizer)\n",
    "    valid_loss_bce, valid_bacc, valid_acc = valid_epoch(valid_loader, model, criterion)\n",
    "    print(f'train loss bce: {train_loss_bce}')\n",
    "    print(f'valid loss bce: {valid_loss_bce}')\n",
    "    print(f'valid round bacc: {valid_bacc[0]}, valid round acc:{valid_acc[0]}')\n",
    "    print(f'valid backhand bacc: {valid_bacc[1]}, valid round acc:{valid_acc[1]}')\n",
    "    print(f'valid ballheight bacc: {valid_bacc[2]}, valid round acc:{valid_acc[2]}')\n",
    "    print(f'valid serve bacc : {valid_bacc[3]}, valid round acc:{valid_acc[3]}')\n",
    "    all_bacc = np.mean(valid_bacc)\n",
    "    print(f'all bacc: {all_bacc}')\n",
    "    \n",
    "    #writer\n",
    "    writer.add_scalar(\"BCE Loss/train\",train_loss_bce, ep)\n",
    "    writer.add_scalar(\"BCE Loss/val\",valid_loss_bce, ep)\n",
    "    writer.add_scalar(\"Round avg recall/val\",valid_bacc[0], ep)\n",
    "    writer.add_scalar(\"Backhand avg recall/val\",valid_bacc[1], ep)\n",
    "    writer.add_scalar(\"Ballheight avg recall/val\",valid_bacc[2], ep)\n",
    "    writer.add_scalar(\"Serve avg recall/val\",valid_bacc[3], ep)\n",
    "    writer.add_scalar(\"All avg recall/val\",all_bacc, ep)\n",
    "    \n",
    "    if all_bacc >= best_score:\n",
    "        best_score= all_bacc\n",
    "        torch.save(model, f\"{CFG['save_model']}/cv{CFG['fold']}_effb0_classification_best_bigwidt_serve.pth\")\n",
    "        print(f'model save at score: {best_score}')\n",
    "        \n",
    "    ## save model every epoch\n",
    "    torch.save(model, f\"{CFG['save_model']}/cv{CFG['fold']}_ep{ep}.pth\")\n",
    "    \n",
    "#     ## adjust lr\n",
    "#     if ep == 50:\n",
    "#         model= torch.load(f\"{CFG['save_model']}/cv{CFG['fold']}_effb0_classification_best.pth\")\n",
    "#         optimizer.param_groups[0]['lr'] = 1e-4\n",
    "#         print('Decrease learning rate to 1e-4!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test serve acc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def serve_inference(df_serve, model, img_list, train_img_size):\n",
    "    model.eval()\n",
    "    serve_dict = {}\n",
    "    hitframe_list = df_serve['HitFrame'].tolist()\n",
    "    videoname_list = df_serve['group'].tolist()\n",
    "    for i in tqdm(range(len(df_serve))):\n",
    "        hitframe = hitframe_list[i]\n",
    "        videoname = videoname_list[i].replace('.','')\n",
    "        img_path = [x for x in img_list if f'{videoname}_A_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "        img_path_B = [x for x in img_list if f'{videoname}_B_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "        img_path.extend(img_path_B)\n",
    "        result = []\n",
    "        for img_name in img_path:\n",
    "            path = f'sorted_player/mix_sorted_player_bigwidth/all/{img_name}'\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            #For label\n",
    "            img = resized_padding(img, train_img_size).transpose(2,0,1) \n",
    "            img = torch.tensor(img/255, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "            output1 = model(img)\n",
    "            result.append(output1.sigmoid()[0,3])\n",
    "#         print(result)\n",
    "        if result[0] > result[1]:\n",
    "            serve_dict[videoname] = {'server':'A', 'image':img_path[0],'score':float(result[0]),'defender':'B','d_score':float(result[1])}\n",
    "        elif result[0] == result[1]:\n",
    "            print(f'{videoname} : A == B')\n",
    "        else:\n",
    "            serve_dict[videoname] = {'server':'B', 'image':img_path[1],'score':float(result[1]), 'defender':'A','d_score':float(result[0])}\n",
    "    return serve_dict\n",
    "def resized_padding(img, train_img_size):\n",
    "    #Resize\n",
    "    h, w = img.shape[:2]\n",
    "    if h >= w:\n",
    "        scale = train_img_size / h\n",
    "        dim = (int(scale * w), train_img_size)\n",
    "    else:\n",
    "        scale = train_img_size / w\n",
    "        dim = (train_img_size, int(scale * h))\n",
    "    resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "    #Padding\n",
    "    left, top = 0, 0\n",
    "    bottom = train_img_size - resized_img.shape[0]\n",
    "    right = train_img_size - resized_img.shape[1]\n",
    "    train_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    return train_img"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## create model\n",
    "if CFG['load_model']:\n",
    "    print(f\"load_model: {CFG['load_model']}\")\n",
    "    model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "else:\n",
    "    model= Customize_Model(CFG['model_name'], CFG['num_classes'])\n",
    "model.to('cuda')\n",
    "\n",
    "## inference serve\n",
    "img_list = os.listdir('sorted_player/mix_sorted_player_bigwidth/all')\n",
    "df= pd.read_csv('hitframe/hitframe.csv')\n",
    "serve_df= df[(df['fold']==CFG['fold']) & (df['ShotSeq'] == 1)]\n",
    "print(f'serve dataset: {len(serve_df)}')\n",
    "serve_dict = serve_inference(serve_df, model, img_list, CFG['img_size'])\n",
    "\n",
    "##evaluate\n",
    "pred_serve = []\n",
    "true_serve = []\n",
    "for index, row in serve_df.iterrows():\n",
    "    if row['Hitter']=='A':\n",
    "        true_serve.append(0)\n",
    "    else:\n",
    "        true_serve.append(1)\n",
    "    \n",
    "    if serve_dict[row['group'].replace('.','')]['server']=='A':\n",
    "        pred_serve.append(0)\n",
    "    else:\n",
    "        pred_serve.append(1)\n",
    "        \n",
    "    if row['Hitter'] != serve_dict[row['group'].replace('.','')]['server']:\n",
    "        print(row['group'])\n",
    "        print(f\"true : {row['Hitter']}\")\n",
    "        print(f\"pred : {serve_dict[row['group'].replace('.','')]['server']}\")\n",
    "        print(f\"pred score : {serve_dict[row['group'].replace('.','')]['score']}\")\n",
    "        print(f\"defender score : {serve_dict[row['group'].replace('.','')]['d_score']}\")\n",
    "print(f\"serve accuracy : {accuracy_score(true_serve, pred_serve)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test round,backhand,ballheight acc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def category_inference(df_val, model, img_list, train_img_size, val_class):\n",
    "    model.eval()\n",
    "    serve_dict = {}\n",
    "    hitframe_list = df_val['HitFrame'].tolist()\n",
    "    videoname_list = df_val['group'].tolist()\n",
    "    hitter_list = df_val['Hitter'].tolist()\n",
    "    is_roundhead = [0 if i==2 else i for i in df_val['RoundHead'].tolist()]\n",
    "    is_backhand = [0 if i==2 else i for i in df_val['Backhand'].tolist()] \n",
    "    ball_height = [0 if i==2 else i for i in df_val['BallHeight'].tolist()] \n",
    "    failure_list, predict_ans = [], []\n",
    "    for i in tqdm(range(len(df_val))):\n",
    "        hitframe = hitframe_list[i]\n",
    "        hitter = hitter_list[i]\n",
    "        videoname = videoname_list[i].replace('.','')\n",
    "        img_path = [x for x in img_list if f'{videoname}_{hitter}_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "        if not img_path:\n",
    "            print(f\"no image on {videoname}_{hitter}_hitframe_{hitframe}\")\n",
    "        path = f'sorted_player/sorted_player_bigwidth/hitter/{img_path[0]}'\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #For label\n",
    "        img = resized_padding(img, train_img_size).transpose(2,0,1) \n",
    "        img = torch.tensor(img/255, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "        output1 = model(img)\n",
    "        output1 = torch.where(output1.sigmoid() >= 0.5, 1, 0)\n",
    "        if val_class == 'round':\n",
    "            if output1[0,0] != is_roundhead[i]:\n",
    "                failure_list.append(f'sorted_player/sorted_player_bigwidth/hitter/{img_path[0]}') \n",
    "                predict_ans.append(int(output1[0,0]))\n",
    "        elif val_class == 'backhand':\n",
    "            if output1[0,1] != is_backhand[i]:\n",
    "                failure_list.append(f'sorted_player/sorted_player_bigwidth/hitter/{img_path[0]}')\n",
    "                predict_ans.append(int(output1[0,1]))\n",
    "        elif val_class == 'ballheight':\n",
    "            if output1[0,2] != ball_height[i]:\n",
    "                failure_list.append(f'sorted_player/sorted_player_bigwidth/hitter/{img_path[0]}')\n",
    "                predict_ans.append(int(output1[0,2]))\n",
    "    print(f\"test for {val_class} class, failure cases number = {len(failure_list)} \")\n",
    "    return {'path':failure_list,'pred_ans':predict_ans}\n",
    "def resized_padding(img, train_img_size):\n",
    "    #Resize\n",
    "    h, w = img.shape[:2]\n",
    "    if h >= w:\n",
    "        scale = train_img_size / h\n",
    "        dim = (int(scale * w), train_img_size)\n",
    "    else:\n",
    "        scale = train_img_size / w\n",
    "        dim = (train_img_size, int(scale * h))\n",
    "    resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "    #Padding\n",
    "    left, top = 0, 0\n",
    "    bottom = train_img_size - resized_img.shape[0]\n",
    "    right = train_img_size - resized_img.shape[1]\n",
    "    train_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    return train_img"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## create model\n",
    "if CFG['load_model']:\n",
    "    print(f\"load_model: {CFG['load_model']}\")\n",
    "    model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "else:\n",
    "    model= Customize_Model(CFG['model_name'], CFG['num_classes'])\n",
    "model.to('cuda')\n",
    "\n",
    "## inference serve\n",
    "img_list = os.listdir('sorted_player/sorted_player_bigwidth/hitter/')\n",
    "df= pd.read_csv('hitframe/hitframe.csv')\n",
    "val_df= df[df['fold']==CFG['fold']]\n",
    "print(f'val dataset: {len(val_df)}')\n",
    "faildict = category_inference(val_df, model, img_list, CFG['img_size'], 'backhand')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(faildict.keys())\n",
    "import shutil\n",
    "destdir = './failcase/backhand/'\n",
    "for i in range(len(faildict['path'])):\n",
    "    img_name = faildict['path'][i].split('/')[-1]\n",
    "#     print(faildict['pred_ans'][i])\n",
    "    shutil.copy(faildict['path'][i], f\"{destdir}pred{faildict['pred_ans'][i]}{img_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badminton_aicup",
   "language": "python",
   "name": "badminton_aicup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
