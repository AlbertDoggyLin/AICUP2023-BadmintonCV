{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4b2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "# from pytorch_toolbelt import losses as L\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "# ## using gpu:1\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895611a3",
   "metadata": {},
   "source": [
    "# CFG serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab086f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold': 0,\n",
    "    'image_path': 0,\n",
    "    'epoch': 100,\n",
    "    'model_name': 'tf_efficientnet_b0_ns',\n",
    "    'num_classes':4,\n",
    "    \n",
    "    'img_size': 224,\n",
    "    'batch_size': 10,\n",
    "\n",
    "#     'load_model': './Model/train/cv0_ep300.pth', #output2已經過sigmoid\n",
    "    'load_model': './weight/cv0_effb0_classification_aug_all_best_bigwidth_serve.pth', #output2還沒經過sigmoid\n",
    "    'save_model': './weight'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e249c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customize_Model(nn.Module):\n",
    "    def __init__(self, model_name, cls):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=True)\n",
    "        \n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        # is_rounded, is_backhand, ball_height, is_serve, locationX, locationY\n",
    "        self.fc = nn.Linear(in_features, cls)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.model(image)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7938bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serve_inference(df_serve, model, img_list, train_img_size):\n",
    "    model.eval()\n",
    "    serve_dict = {}\n",
    "    hitframe_list = df_serve['HitFrame'].tolist()\n",
    "    videoname_list = df_serve['VideoName'].tolist()\n",
    "    for i in tqdm(range(len(df_serve))):\n",
    "        hitframe = hitframe_list[i]-1\n",
    "        videoname = videoname_list[i].replace('.','')\n",
    "        img_path = [x for x in img_list if f'{videoname}_A_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "        img_path_B = [x for x in img_list if f'{videoname}_B_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "        img_path.extend(img_path_B)\n",
    "        result = []\n",
    "        for img_name in img_path:\n",
    "#             print(img_name)\n",
    "            path = f'Test_data/val_set_bigwidth/{img_name}'\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            #For label\n",
    "            img = resized_padding(img, train_img_size).transpose(2,0,1) \n",
    "            img = torch.tensor(img/255, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "            output = model(img)\n",
    "            result.append(output.sigmoid()[0,3])\n",
    "\n",
    "        if result[0] > result[1]:\n",
    "            serve_dict[videoname] = {'server':'A', 'image':img_path[0],'score':float(result[0])}\n",
    "        elif result[0] == result[1]:\n",
    "            print(f'{videoname} : A == B')\n",
    "        else:\n",
    "            serve_dict[videoname] = {'server':'B', 'image':img_path[1],'score':float(result[1])}\n",
    "    return serve_dict\n",
    "\n",
    "def result_inference(df, category_model, regression_model, img_list, train_img_size):\n",
    "    category_model.eval()\n",
    "    regression_model.eval()\n",
    "    result_dict = {}\n",
    "    hitframe_list = df['HitFrame'].tolist()\n",
    "    videoname_list = df['VideoName'].tolist()\n",
    "    hitter_list = df['Hitter'].tolist()\n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            hitframe = hitframe_list[i]-1\n",
    "            videoname = videoname_list[i].replace('.','')\n",
    "            hitter = hitter_list[i]\n",
    "            if hitter == 'A':\n",
    "                defender = 'B'\n",
    "            else:\n",
    "                defender = 'A'\n",
    "            img_path = []\n",
    "            img_path.extend([x for x in img_list if f'{videoname}_{hitter}_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe])\n",
    "            img_path.extend([x for x in img_list if f'{videoname}_{defender}_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe])\n",
    "\n",
    "            category = []\n",
    "            location_xy = []\n",
    "            for j in range(len(img_path)):\n",
    "                img = cv2.imread(f'Test_data/val_set_bigwidth/{img_path[j]}')\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                h, w = img.shape[:2]\n",
    "                img = resized_padding(img, train_img_size).transpose(2,0,1) \n",
    "                img = torch.tensor(img/255, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "                x0 = int(img_path[j].split('_')[4][1:])\n",
    "                y0 = int(img_path[j].split('_')[5][1:-4])\n",
    "                # is_rounded, is_backhand, ball_height, is_serve, locationX, locationY\n",
    "                output1 = category_model(img)\n",
    "                output2 = regression_model(img)\n",
    "                output1 = torch.where(output1.sigmoid() >= 0.5, 1, 2)\n",
    "                category.append(output1[0,:])\n",
    "                output2 = output2.sigmoid() * max(h,w)\n",
    "                location_xy.append([int(output2[0,0]) + x0, int(output2[0,1]) + y0])\n",
    "\n",
    "\n",
    "            result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}'] = {'is_rounded':int(category[0][0]),\n",
    "                                                                   'is_backhand':int(category[0][1]),\n",
    "                                                                   'ball_height':int(category[0][2]),\n",
    "                                                                   'hitterlocationX':int(location_xy[0][0]),\n",
    "                                                                   'hitterlocationY':int(location_xy[0][1]),\n",
    "                                                                   'defenderlocationX':int(location_xy[1][0]),\n",
    "                                                                   'defenderlocationY':int(location_xy[1][1]),\n",
    "                                                                  }\n",
    "        except Exception as e:\n",
    "            print(i)\n",
    "            print(e)\n",
    "    return result_dict\n",
    "\n",
    "def resized_padding(img, train_img_size):\n",
    "        #Resize\n",
    "        h, w = img.shape[:2]\n",
    "        if h >= w:\n",
    "            scale = train_img_size / h\n",
    "            dim = (int(scale * w), train_img_size)\n",
    "        else:\n",
    "            scale = train_img_size / w\n",
    "            dim = (train_img_size, int(scale * h))\n",
    "        resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "        #Padding\n",
    "        left, top = 0, 0\n",
    "        bottom = train_img_size - resized_img.shape[0]\n",
    "        right = train_img_size - resized_img.shape[1]\n",
    "        train_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        return train_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f11d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitter_inference(df, model, img_list, train_img_size):\n",
    "    model.eval()\n",
    "    hitter_dict = {}\n",
    "    hitframe_list = df['HitFrame'].tolist()\n",
    "    videoname_list = df['VideoName'].tolist()\n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            hitframe = hitframe_list[i]-1\n",
    "            videoname = videoname_list[i].replace('.','')\n",
    "            img_path = [x for x in img_list if f'{videoname}_A_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "            img_path_B = [x for x in img_list if f'{videoname}_B_hitframe_{hitframe}' in x and int(x.split('_')[3])==hitframe]\n",
    "            img_path.extend(img_path_B)\n",
    "            result = []\n",
    "            for img_name in img_path:\n",
    "    #             print(img_name)\n",
    "                path = f'Test_data/val_set_bigwidth/{img_name}'\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                #For label\n",
    "                img = resized_padding(img, train_img_size).transpose(2,0,1) \n",
    "                img = torch.tensor(img/255, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "                output = model(img)\n",
    "                result.append(output.sigmoid()[0])\n",
    "\n",
    "            if result[0] > result[1]:\n",
    "                hitter_dict[f\"{videoname}_{hitframe}\"] = {'hitter':'A', 'image':img_path[0],'score':float(result[0])}\n",
    "            elif result[0] == result[1]:\n",
    "                print(f'{videoname} : A == B')\n",
    "            else:\n",
    "                hitter_dict[f\"{videoname}_{hitframe}\"] = {'hitter':'B', 'image':img_path[1],'score':float(result[1])}\n",
    "        except Exception as e:\n",
    "            print(i)\n",
    "            print(e)\n",
    "    return hitter_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db079a",
   "metadata": {},
   "source": [
    "# Output hitter (cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "df = pd.read_csv('test_submission_new.csv')\n",
    "out_df = df.copy()\n",
    "img_list = os.listdir('Test_data/val_set_bigwidth/')\n",
    "bad_result = []\n",
    "for i in range(5):\n",
    "    CFG['load_model'] = f'./weight/cv{i}_effb0_classification_aug_all_best_bigwidth_hitter.pth'\n",
    "    print(f\"hitter load_model: {CFG['load_model']}\")\n",
    "    hitter_model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "    hitter_dict = hitter_inference(df, hitter_model, img_list, CFG['img_size'])\n",
    "    ##output test_submission_hitter\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            hitter_list = ['A','B']\n",
    "            videoname = row['VideoName'].replace('.','')\n",
    "            hitframe = row['HitFrame']-1\n",
    "            out_df.at[index, 'Hitter'] = hitter_dict[f\"{videoname}_{hitframe}\"]['hitter']\n",
    "        except Exception as e:\n",
    "            print(index)\n",
    "            print(e)\n",
    "    out_df.to_csv(f'test_submission_hitter_cv{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf7770",
   "metadata": {},
   "source": [
    "# Intergate hitter 5 fold result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d7083370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00021.mp4 5\n",
      "['B', 'A', 'B', 'B', 'B'] B\n",
      "00341.mp4 0\n",
      "['B', 'A', 'B', 'B', 'B'] B\n"
     ]
    }
   ],
   "source": [
    "df_hitter = pd.read_csv('test_submission_new.csv')\n",
    "df_cv0_hitter = pd.read_csv('test_submission_hitter_cv0.csv')\n",
    "df_cv1_hitter = pd.read_csv('test_submission_hitter_cv1.csv')\n",
    "df_cv2_hitter = pd.read_csv('test_submission_hitter_cv2.csv')\n",
    "df_cv3_hitter = pd.read_csv('test_submission_hitter_cv3.csv')\n",
    "df_cv4_hitter = pd.read_csv('test_submission_hitter_cv4.csv')\n",
    "cv_df_list = [df_cv0_hitter, df_cv1_hitter, df_cv2_hitter, df_cv3_hitter, df_cv4_hitter]\n",
    "for index, row in df_hitter.iterrows():\n",
    "    hitter_list = [i.at[index, 'Hitter'] for i in cv_df_list]\n",
    "    if row['ShotSeq'] == 1 and len(set(hitter_list))!=1:\n",
    "        print(row['VideoName'],row['HitFrame'])\n",
    "        print(hitter_list, max(set(hitter_list),key=hitter_list.count))\n",
    "    df_hitter.at[index, 'Hitter'] = max(set(hitter_list),key=hitter_list.count)\n",
    "df_hitter.to_csv(f'test_submission_hitter_5fold.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9424d17b",
   "metadata": {},
   "source": [
    "# post process on low confidience score (hitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "04b3e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_submission_hitter_5fold.csv')\n",
    "out_df = df.copy()\n",
    "for index, row in df.iterrows():\n",
    "    if index > 0 and index < len(out_df)-1:\n",
    "        if out_df.at[index, 'Hitter'] == out_df.at[index-1, 'Hitter'] and out_df.at[index, 'VideoName'] == out_df.at[index-1, 'VideoName']:\n",
    "            now_video = out_df.at[index, 'VideoName'].replace('.','')\n",
    "            pre_video = out_df.at[index-1, 'VideoName'].replace('.','')\n",
    "            now_hitframe = out_df.at[index, 'HitFrame']-1\n",
    "            pre_hitframe = out_df.at[index-1, 'HitFrame']-1\n",
    "\n",
    "            if hitter_dict[f\"{now_video}_{now_hitframe}\"]['score'] > hitter_dict[f\"{pre_video}_{pre_hitframe}\"]['score'] and\\\n",
    "    hitter_dict[f\"{pre_video}_{pre_hitframe}\"]['score'] < 0.5:\n",
    "                if out_df.at[index-1, 'Hitter']=='B':\n",
    "                    out_df.at[index-1, 'Hitter'] = 'A'\n",
    "                else:\n",
    "                    out_df.at[index-1, 'Hitter']='B'\n",
    "            elif hitter_dict[f\"{now_video}_{now_hitframe}\"]['score'] < hitter_dict[f\"{pre_video}_{pre_hitframe}\"]['score'] and\\\n",
    "            hitter_dict[f\"{now_video}_{now_hitframe}\"]['score'] < 0.5:\n",
    "                if out_df.at[index, 'Hitter']=='B':\n",
    "                    out_df.at[index, 'Hitter'] = 'A'\n",
    "                else:\n",
    "                    out_df.at[index, 'Hitter']='B'\n",
    "    out_df.to_csv(f'test_submission_hitter_5fold.csv', index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550522d",
   "metadata": {},
   "source": [
    "# Fill in missing frames in the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3381d9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00007.mp4 59\n",
      "before dataframe : 3891\n",
      "after dataframe : 3892\n",
      "00020.mp4 214\n",
      "before dataframe : 3892\n",
      "after dataframe : 3893\n",
      "00047.mp4 505\n",
      "before dataframe : 3893\n",
      "after dataframe : 3894\n",
      "00047.mp4 532\n",
      "before dataframe : 3894\n",
      "after dataframe : 3895\n",
      "00049.mp4 170\n",
      "before dataframe : 3895\n",
      "after dataframe : 3896\n",
      "00053.mp4 93\n",
      "before dataframe : 3896\n",
      "after dataframe : 3897\n",
      "00055.mp4 346\n",
      "before dataframe : 3897\n",
      "after dataframe : 3898\n",
      "00059.mp4 301\n",
      "before dataframe : 3898\n",
      "after dataframe : 3899\n",
      "00064.mp4 45\n",
      "before dataframe : 3899\n",
      "after dataframe : 3900\n",
      "00064.mp4 232\n",
      "before dataframe : 3900\n",
      "after dataframe : 3901\n",
      "00064.mp4 336\n",
      "before dataframe : 3901\n",
      "after dataframe : 3902\n",
      "00072.mp4 136\n",
      "before dataframe : 3902\n",
      "after dataframe : 3903\n",
      "00079.mp4 126\n",
      "before dataframe : 3903\n",
      "after dataframe : 3904\n",
      "00080.mp4 296\n",
      "before dataframe : 3904\n",
      "after dataframe : 3905\n",
      "00091.mp4 228\n",
      "before dataframe : 3905\n",
      "after dataframe : 3906\n",
      "00091.mp4 335\n",
      "before dataframe : 3906\n",
      "after dataframe : 3907\n",
      "00099.mp4 160\n",
      "before dataframe : 3907\n",
      "after dataframe : 3908\n",
      "00114.mp4 659\n",
      "before dataframe : 3908\n",
      "after dataframe : 3909\n",
      "00119.mp4 114\n",
      "before dataframe : 3909\n",
      "after dataframe : 3910\n",
      "00137.mp4 55\n",
      "before dataframe : 3910\n",
      "after dataframe : 3911\n",
      "00147.mp4 844\n",
      "before dataframe : 3911\n",
      "after dataframe : 3912\n",
      "00151.mp4 189\n",
      "before dataframe : 3912\n",
      "after dataframe : 3913\n",
      "00151.mp4 260\n",
      "before dataframe : 3913\n",
      "after dataframe : 3914\n",
      "00157.mp4 172\n",
      "before dataframe : 3914\n",
      "after dataframe : 3915\n",
      "00186.mp4 163\n",
      "before dataframe : 3915\n",
      "after dataframe : 3916\n",
      "00187.mp4 280\n",
      "before dataframe : 3916\n",
      "after dataframe : 3917\n",
      "00190.mp4 113\n",
      "before dataframe : 3917\n",
      "after dataframe : 3918\n",
      "00190.mp4 294\n",
      "before dataframe : 3918\n",
      "after dataframe : 3919\n",
      "00193.mp4 138\n",
      "before dataframe : 3919\n",
      "after dataframe : 3920\n",
      "00193.mp4 164\n",
      "before dataframe : 3920\n",
      "after dataframe : 3921\n",
      "00194.mp4 130\n",
      "before dataframe : 3921\n",
      "after dataframe : 3922\n",
      "00194.mp4 159\n",
      "before dataframe : 3922\n",
      "after dataframe : 3923\n",
      "00209.mp4 61\n",
      "before dataframe : 3923\n",
      "after dataframe : 3924\n",
      "00215.mp4 413\n",
      "before dataframe : 3924\n",
      "after dataframe : 3925\n",
      "00218.mp4 209\n",
      "before dataframe : 3925\n",
      "after dataframe : 3926\n",
      "00220.mp4 257\n",
      "before dataframe : 3926\n",
      "after dataframe : 3927\n",
      "00222.mp4 119\n",
      "before dataframe : 3927\n",
      "after dataframe : 3928\n",
      "00226.mp4 389\n",
      "before dataframe : 3928\n",
      "after dataframe : 3929\n",
      "00227.mp4 124\n",
      "before dataframe : 3929\n",
      "after dataframe : 3930\n",
      "00227.mp4 185\n",
      "before dataframe : 3930\n",
      "after dataframe : 3931\n",
      "00230.mp4 284\n",
      "before dataframe : 3931\n",
      "after dataframe : 3932\n",
      "00230.mp4 295\n",
      "before dataframe : 3932\n",
      "after dataframe : 3933\n",
      "00230.mp4 326\n",
      "before dataframe : 3933\n",
      "after dataframe : 3934\n",
      "00238.mp4 103\n",
      "before dataframe : 3934\n",
      "after dataframe : 3935\n",
      "00240.mp4 54\n",
      "before dataframe : 3935\n",
      "after dataframe : 3936\n",
      "00242.mp4 398\n",
      "before dataframe : 3936\n",
      "after dataframe : 3937\n",
      "00244.mp4 165\n",
      "before dataframe : 3937\n",
      "after dataframe : 3938\n",
      "00257.mp4 48\n",
      "before dataframe : 3938\n",
      "after dataframe : 3939\n",
      "00262.mp4 96\n",
      "before dataframe : 3939\n",
      "after dataframe : 3940\n",
      "00276.mp4 192\n",
      "before dataframe : 3940\n",
      "after dataframe : 3941\n",
      "00276.mp4 284\n",
      "before dataframe : 3941\n",
      "after dataframe : 3942\n",
      "00278.mp4 766\n",
      "before dataframe : 3942\n",
      "after dataframe : 3943\n",
      "00286.mp4 120\n",
      "before dataframe : 3943\n",
      "after dataframe : 3944\n",
      "00286.mp4 242\n",
      "before dataframe : 3944\n",
      "after dataframe : 3945\n",
      "00286.mp4 281\n",
      "before dataframe : 3945\n",
      "after dataframe : 3946\n",
      "00290.mp4 106\n",
      "before dataframe : 3946\n",
      "after dataframe : 3947\n",
      "00302.mp4 253\n",
      "before dataframe : 3947\n",
      "after dataframe : 3948\n",
      "00316.mp4 289\n",
      "before dataframe : 3948\n",
      "after dataframe : 3949\n",
      "00316.mp4 566\n",
      "before dataframe : 3949\n",
      "after dataframe : 3950\n",
      "00316.mp4 1085\n",
      "before dataframe : 3950\n",
      "after dataframe : 3951\n",
      "00325.mp4 256\n",
      "before dataframe : 3951\n",
      "after dataframe : 3952\n",
      "00325.mp4 688\n",
      "before dataframe : 3952\n",
      "after dataframe : 3953\n",
      "00327.mp4 130\n",
      "before dataframe : 3953\n",
      "after dataframe : 3954\n",
      "00329.mp4 87\n",
      "before dataframe : 3954\n",
      "after dataframe : 3955\n",
      "00333.mp4 321\n",
      "before dataframe : 3955\n",
      "after dataframe : 3956\n",
      "00338.mp4 222\n",
      "before dataframe : 3956\n",
      "after dataframe : 3957\n",
      "00339.mp4 183\n",
      "before dataframe : 3957\n",
      "after dataframe : 3958\n",
      "00341.mp4 20\n",
      "before dataframe : 3958\n",
      "after dataframe : 3959\n",
      "00345.mp4 55\n",
      "before dataframe : 3959\n",
      "after dataframe : 3960\n",
      "00345.mp4 182\n",
      "before dataframe : 3960\n",
      "after dataframe : 3961\n",
      "00353.mp4 82\n",
      "before dataframe : 3961\n",
      "after dataframe : 3962\n",
      "00353.mp4 135\n",
      "before dataframe : 3962\n",
      "after dataframe : 3963\n",
      "00353.mp4 147\n",
      "before dataframe : 3963\n",
      "after dataframe : 3964\n",
      "00364.mp4 211\n",
      "before dataframe : 3964\n",
      "after dataframe : 3965\n",
      "00364.mp4 233\n",
      "before dataframe : 3965\n",
      "after dataframe : 3966\n",
      "00366.mp4 259\n",
      "before dataframe : 3966\n",
      "after dataframe : 3967\n",
      "00366.mp4 406\n",
      "before dataframe : 3967\n",
      "after dataframe : 3968\n",
      "00368.mp4 30\n",
      "before dataframe : 3968\n",
      "after dataframe : 3969\n",
      "00375.mp4 212\n",
      "before dataframe : 3969\n",
      "after dataframe : 3970\n",
      "00379.mp4 271\n",
      "before dataframe : 3970\n",
      "after dataframe : 3971\n",
      "00381.mp4 95\n",
      "before dataframe : 3971\n",
      "after dataframe : 3972\n",
      "00383.mp4 77\n",
      "before dataframe : 3972\n",
      "after dataframe : 3973\n"
     ]
    }
   ],
   "source": [
    "## find middle_complete_case:\n",
    "df = pd.read_csv('test_submission_hitter_5fold.csv')\n",
    "middle_complete_list = []\n",
    "for index, row in df.iterrows():\n",
    "    if index > 0 and index < len(out_df)-1:\n",
    "        if df.at[index, 'Hitter'] == df.at[index-1, 'Hitter'] and df.at[index, 'VideoName'] == df.at[index-1, 'VideoName']:\n",
    "            middle_complete_list.append(f\"{df.at[index, 'VideoName']}/{df.at[index, 'HitFrame']}\")\n",
    "\n",
    "## complete middle row            \n",
    "for complete_case in middle_complete_list:\n",
    "    df = pd.read_csv('test_submission_hitter_5fold.csv')\n",
    "    out_df = df.copy()\n",
    "    videoname, hitframe = complete_case.split('/')[0],int(complete_case.split('/')[1]) \n",
    "    print(videoname, hitframe)\n",
    "    print(f'before dataframe : {len(df)}')\n",
    "    row_to_insert = pd.DataFrame(df[(df['VideoName'] == videoname) & (df['HitFrame']==hitframe)].iloc[0,:]).T\n",
    "    row_shotseq = row_to_insert['ShotSeq'].values[0]\n",
    "    row_idx = row_to_insert.index.values[0]\n",
    "    row_to_insert['HitFrame'] = int((df.loc[row_idx+1, 'HitFrame'] + df.loc[row_idx-1, 'HitFrame'])/2)\n",
    "    row_to_insert['BallType'] = 6\n",
    "    if row_to_insert['Hitter'].values[0] == 'B':\n",
    "        row_to_insert['Hitter'] = 'A'\n",
    "    else:\n",
    "        row_to_insert['Hitter'] = 'B'\n",
    "    out_df.loc[(df['ShotSeq'] >= row_shotseq) & (df['VideoName']==videoname), 'ShotSeq'] += 1\n",
    "    df = pd.concat([out_df.iloc[:row_idx], row_to_insert, out_df.iloc[row_idx:]])\n",
    "    print(f'after dataframe : {len(df)}')\n",
    "    df.to_csv(f'test_submission_hitter_5fold.csv', index=False)   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a71605",
   "metadata": {},
   "source": [
    "# Output server submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7db2e8b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serve load_model: ./Model/train/cv0_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 399/399 [00:13<00:00, 30.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00013mp4 video serve score < 0.5, socre = 0.45136943459510803\n",
      "00022mp4 video serve score < 0.5, socre = 0.15839190781116486\n",
      "00045mp4 video serve score < 0.5, socre = 0.15443448722362518\n",
      "00289mp4 video serve score < 0.5, socre = 0.1825794279575348\n",
      "00307mp4 video serve score < 0.5, socre = 0.3717375099658966\n",
      "00340mp4 video serve score < 0.5, socre = 0.015059070661664009\n",
      "00341mp4 video serve score < 0.5, socre = 0.00817057304084301\n",
      "serve load_model: ./Model/train/cv1_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 399/399 [00:13<00:00, 29.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00045mp4 video serve score < 0.5, socre = 0.4807322323322296\n",
      "00059mp4 video serve score < 0.5, socre = 0.05156675726175308\n",
      "00166mp4 video serve score < 0.5, socre = 0.13144806027412415\n",
      "00307mp4 video serve score < 0.5, socre = 0.045414552092552185\n",
      "00319mp4 video serve score < 0.5, socre = 0.3412332832813263\n",
      "00332mp4 video serve score < 0.5, socre = 0.4367683529853821\n",
      "00340mp4 video serve score < 0.5, socre = 0.024085111916065216\n",
      "00341mp4 video serve score < 0.5, socre = 0.23644788563251495\n",
      "00382mp4 video serve score < 0.5, socre = 0.31783342361450195\n",
      "serve load_model: ./Model/train/cv2_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 399/399 [00:13<00:00, 29.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00022mp4 video serve score < 0.5, socre = 0.014766699634492397\n",
      "00045mp4 video serve score < 0.5, socre = 0.013635795563459396\n",
      "00112mp4 video serve score < 0.5, socre = 0.009232576005160809\n",
      "00169mp4 video serve score < 0.5, socre = 0.46846887469291687\n",
      "00225mp4 video serve score < 0.5, socre = 0.3018782138824463\n",
      "00270mp4 video serve score < 0.5, socre = 0.13653548061847687\n",
      "00289mp4 video serve score < 0.5, socre = 0.16608285903930664\n",
      "00307mp4 video serve score < 0.5, socre = 0.36543241143226624\n",
      "00332mp4 video serve score < 0.5, socre = 0.03454652056097984\n",
      "00340mp4 video serve score < 0.5, socre = 0.032552242279052734\n",
      "00341mp4 video serve score < 0.5, socre = 0.00884865503758192\n",
      "00382mp4 video serve score < 0.5, socre = 0.001373715465888381\n",
      "serve load_model: ./Model/train/cv3_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 399/399 [00:13<00:00, 30.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00022mp4 video serve score < 0.5, socre = 0.07900693267583847\n",
      "00045mp4 video serve score < 0.5, socre = 0.015221565030515194\n",
      "00112mp4 video serve score < 0.5, socre = 0.07775895297527313\n",
      "00166mp4 video serve score < 0.5, socre = 0.3405083119869232\n",
      "00289mp4 video serve score < 0.5, socre = 0.10652220249176025\n",
      "00307mp4 video serve score < 0.5, socre = 0.11979810148477554\n",
      "00319mp4 video serve score < 0.5, socre = 0.23798814415931702\n",
      "00332mp4 video serve score < 0.5, socre = 0.21312138438224792\n",
      "00340mp4 video serve score < 0.5, socre = 0.07914169132709503\n",
      "00341mp4 video serve score < 0.5, socre = 0.32942044734954834\n",
      "00342mp4 video serve score < 0.5, socre = 0.1987515538930893\n",
      "00344mp4 video serve score < 0.5, socre = 0.3882361650466919\n",
      "serve load_model: ./Model/train/cv4_effb0_classification_aug_all_best_bigwidth_serve.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 399/399 [00:13<00:00, 29.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00022mp4 video serve score < 0.5, socre = 0.0495590940117836\n",
      "00045mp4 video serve score < 0.5, socre = 0.0014678918523713946\n",
      "00059mp4 video serve score < 0.5, socre = 0.1255064159631729\n",
      "00082mp4 video serve score < 0.5, socre = 0.3617948889732361\n",
      "00101mp4 video serve score < 0.5, socre = 0.4782799184322357\n",
      "00112mp4 video serve score < 0.5, socre = 0.01999046839773655\n",
      "00166mp4 video serve score < 0.5, socre = 0.011453998275101185\n",
      "00169mp4 video serve score < 0.5, socre = 0.21290040016174316\n",
      "00270mp4 video serve score < 0.5, socre = 0.0040261209942400455\n",
      "00281mp4 video serve score < 0.5, socre = 0.47474342584609985\n",
      "00294mp4 video serve score < 0.5, socre = 0.27410900592803955\n",
      "00307mp4 video serve score < 0.5, socre = 0.038660503923892975\n",
      "00319mp4 video serve score < 0.5, socre = 0.1979702264070511\n",
      "00332mp4 video serve score < 0.5, socre = 0.26465389132499695\n",
      "00340mp4 video serve score < 0.5, socre = 0.059571560472249985\n",
      "00341mp4 video serve score < 0.5, socre = 0.10714758932590485\n",
      "00382mp4 video serve score < 0.5, socre = 0.45147958397865295\n"
     ]
    }
   ],
   "source": [
    "## create model\n",
    "df = pd.read_csv('test_submission_new.csv')\n",
    "df_serve = df[df['ShotSeq'] == 1]\n",
    "img_list = os.listdir('Test_data/val_set_bigwidth/')\n",
    "bad_result = []\n",
    "for i in range(5):\n",
    "    CFG['load_model'] = f'./weight/cv{i}_effb0_classification_aug_all_best_bigwidth_serve.pth'\n",
    "    print(f\"serve load_model: {CFG['load_model']}\")\n",
    "    serve_model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "    serve_dict = serve_inference(df_serve, serve_model, img_list, CFG['img_size'])\n",
    "    ##output test_submission_hitter\n",
    "    for index, row in df.iterrows():\n",
    "        hitter_list = ['A','B']\n",
    "        videoname = row['VideoName'].replace('.','')\n",
    "        shotseq = row['ShotSeq']\n",
    "        if shotseq == 1 and serve_dict[videoname]['score']<0.5:\n",
    "            print(f'{videoname} video serve score < 0.5, socre = {serve_dict[videoname][\"score\"]}')\n",
    "            bad_result.append(videoname)\n",
    "        if shotseq % 2 == 1:\n",
    "            df.at[index, 'Hitter'] = serve_dict[videoname]['server']\n",
    "        else:\n",
    "            hitter_list.remove(serve_dict[videoname]['server'])\n",
    "            df.at[index, 'Hitter'] = hitter_list[0]\n",
    "    df.to_csv(f'test_submission_server_cv{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4607a",
   "metadata": {},
   "source": [
    "# intergrate serve 5-fold result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1d0890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00022.mp4\n",
      "['B', 'A', 'A', 'A', 'A'] A\n",
      "00166.mp4\n",
      "['A', 'A', 'B', 'B', 'A'] A\n",
      "00270.mp4\n",
      "['A', 'A', 'A', 'B', 'B'] A\n",
      "00307.mp4\n",
      "['B', 'A', 'A', 'B', 'B'] B\n",
      "00332.mp4\n",
      "['A', 'A', 'B', 'A', 'B'] A\n",
      "00340.mp4\n",
      "['A', 'B', 'A', 'A', 'A'] A\n",
      "00341.mp4\n",
      "['A', 'B', 'A', 'A', 'B'] A\n",
      "00382.mp4\n",
      "['A', 'B', 'B', 'A', 'A'] A\n"
     ]
    }
   ],
   "source": [
    "df_serve = pd.read_csv('test_submission_new.csv')\n",
    "df_cv0_serve = pd.read_csv('test_submission_server_cv0.csv')\n",
    "df_cv1_serve = pd.read_csv('test_submission_server_cv1.csv')\n",
    "df_cv2_serve = pd.read_csv('test_submission_server_cv2.csv')\n",
    "df_cv3_serve = pd.read_csv('test_submission_server_cv3.csv')\n",
    "df_cv4_serve = pd.read_csv('test_submission_server_cv4.csv')\n",
    "cv_df_list = [df_cv0_serve, df_cv1_serve, df_cv2_serve, df_cv3_serve, df_cv4_serve]\n",
    "for index, row in df_serve.iterrows():\n",
    "    serve_list = [i.at[index, 'Hitter'] for i in cv_df_list]\n",
    "    if row['ShotSeq'] == 1 and len(set(serve_list))!=1:\n",
    "        print(row['VideoName'])\n",
    "        print(serve_list, max(set(serve_list),key=serve_list.count))\n",
    "    df_serve.at[index, 'Hitter'] = max(set(serve_list),key=serve_list.count)\n",
    "#         print(f\"{videoname}_{hitter}_hitframe_{hitframe}:{result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']}\")\n",
    "df_serve.to_csv(f'test_submission_server_5fold.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ac6da",
   "metadata": {},
   "source": [
    "# Fill in missing frames at the beginning of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "510b55c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find missing frames video \n",
    "serve_df = pd.read_csv('test_submission_server_5fold.csv.csv')\n",
    "hitter_df = pd.read_csv('test_submission_hitter_5fold.csv')\n",
    "serve_df = serve_df[serve_df[\"ShotSeq\"]==1].reset_index()\n",
    "hitter_df = hitter_df[hitter_df[\"ShotSeq\"]==1].reset_index()\n",
    "bad_result = []\n",
    "for index, row in serve_df.iterrows():\n",
    "    if row['Hitter'] != hitter_df.at[index, 'Hitter']:\n",
    "        print(f\"serve model {row['VideoName']} server {row['Hitter']} \") \n",
    "        print(f\"hitter model {hitter_df.at[index, 'VideoName']} hitter {hitter_df.at[index, 'Hitter']} \") \n",
    "        bad_result.append(row['VideoName'])\n",
    "##fill in        \n",
    "for videoname in bad_result:\n",
    "    df = pd.read_csv('test_submission_hitter_5fold.csv')\n",
    "    out_df = df.copy()\n",
    "    print(f'before dataframe : {len(df)}')\n",
    "    print(df[df['VideoName']==videoname])\n",
    "    out_df.loc[df['VideoName']==videoname, 'ShotSeq'] += 1 \n",
    "    row_to_insert = pd.DataFrame(df[(df['ShotSeq'] == 1) & (df['VideoName']==videoname)].iloc[0,:]).T\n",
    "    if row_to_insert['Hitter'].values[0] == 'B':\n",
    "        row_to_insert['Hitter'] = 'A'\n",
    "    else:\n",
    "        row_to_insert['Hitter'] = 'B'\n",
    "    row_idx = row_to_insert.index.values[0]\n",
    "    row_to_insert['HitFrame'] = max(0,df.loc[row_idx, 'HitFrame']-(df.loc[row_idx+1, 'HitFrame']-df.loc[row_idx, 'HitFrame']))\n",
    "    row_to_insert['BallType'] = 1\n",
    "    df = pd.concat([out_df.iloc[:row_idx], row_to_insert, out_df.iloc[row_idx:]])#.reset_index(drop=True)\n",
    "    print(f'after dataframe : {len(df)}')\n",
    "    print(df[df['VideoName']==videoname])\n",
    "    df.to_csv(f'test_submission_hitter_5fold.csv', index=False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d89f5a",
   "metadata": {},
   "source": [
    "# Output hitter+category submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f3eface5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression load_model: ./Model/cv0_effb0_location_bestv2.pth\n",
      "category load_model: ./Model/train/cv0_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████                    | 2977/3997 [03:13<00:59, 17.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3997/3997 [04:19<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "'00304mp4_Z_hitframe_459'\n",
      "category load_model: ./Model/train/cv1_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████                    | 2977/3997 [02:55<00:54, 18.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3997/3997 [03:56<00:00, 16.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "'00304mp4_Z_hitframe_459'\n",
      "category load_model: ./Model/train/cv2_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████                    | 2977/3997 [02:55<00:55, 18.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3997/3997 [03:56<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "'00304mp4_Z_hitframe_459'\n",
      "category load_model: ./Model/train/cv3_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████                    | 2977/3997 [02:55<00:55, 18.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3997/3997 [03:55<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "'00304mp4_Z_hitframe_459'\n",
      "category load_model: ./Model/train/cv4_effb0_classification_aug_all_best_bigwidth.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████                    | 2977/3997 [02:55<00:53, 18.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3997/3997 [03:55<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "'00304mp4_Z_hitframe_459'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test_submission_hitter_5fold.csv')\n",
    "img_list = os.listdir('Test_data/val_set_bigwidth/')\n",
    "\n",
    "CFG['load_model'] = './weight/cv0_effb0_location_bestv2.pth'\n",
    "print(f\"regression load_model: {CFG['load_model']}\")\n",
    "regression_model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "\n",
    "for i in range(5):\n",
    "    CFG['load_model'] = f'./weight/cv{i}_effb0_classification_aug_all_best_bigwidth.pth'\n",
    "    print(f\"category load_model: {CFG['load_model']}\")\n",
    "    category_model= torch.load(CFG['load_model'], map_location= 'cuda')\n",
    "    result_dict = result_inference(df, category_model, regression_model, img_list, CFG['img_size'])\n",
    "    ##output test_submission_hitter_category\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            videoname = row['VideoName'].replace('.','')\n",
    "            hitframe = row['HitFrame']-1\n",
    "            hitter = row['Hitter']\n",
    "            df.at[index, 'RoundHead'] = result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']['is_rounded']\n",
    "            df.at[index, 'Backhand'] = result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']['is_backhand']\n",
    "            df.at[index, 'BallHeight'] = result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']['ball_height']\n",
    "            if index != 0 and df.at[index, 'ShotSeq'] == 1:\n",
    "                winner = ['A','B']\n",
    "                winner.remove(df.at[index-1, 'Hitter'])\n",
    "                df.at[index-1, 'Winner'] = winner[0]\n",
    "            elif index == len(df) - 1:\n",
    "                winner = ['A','B']\n",
    "                winner.remove(df.at[index, 'Hitter'])\n",
    "                df.at[index, 'Winner'] = winner[0]\n",
    "#                 print(f\"{videoname}_{hitter}_hitframe_{hitframe}:{result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']}\")\n",
    "        except Exception as e:\n",
    "            print(index)\n",
    "            print(e)\n",
    "    df.to_csv(f'test_submission_hitter_category_cv{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a004cf8",
   "metadata": {},
   "source": [
    "# intergrate category 5-fold result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "bb811a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = pd.read_csv('test_submission_hitter_5fold.csv')\n",
    "df_cv0_category = pd.read_csv('test_submission_hitter_category_cv0.csv')\n",
    "df_cv1_category = pd.read_csv('test_submission_hitter_category_cv1.csv')\n",
    "df_cv2_category = pd.read_csv('test_submission_hitter_category_cv2.csv')\n",
    "df_cv3_category = pd.read_csv('test_submission_hitter_category_cv3.csv')\n",
    "df_cv4_category = pd.read_csv('test_submission_hitter_category_cv4.csv')\n",
    "cv_df_list = [df_cv0_category, df_cv1_category, df_cv2_category, df_cv3_category, df_cv4_category]\n",
    "for index, row in df_category.iterrows():\n",
    "        round_list = [i.at[index, 'RoundHead'] for i in cv_df_list]\n",
    "        backhand_list = [i.at[index, 'Backhand'] for i in cv_df_list]\n",
    "        ballheight_list = [i.at[index, 'BallHeight'] for i in cv_df_list]\n",
    "        winner_list = [i.at[index, 'Winner'] for i in cv_df_list]\n",
    "        df_category.at[index, 'RoundHead'] = max(set(round_list),key=round_list.count)\n",
    "        df_category.at[index, 'Backhand'] = max(set(backhand_list),key=backhand_list.count)\n",
    "        df_category.at[index, 'BallHeight'] = max(set(ballheight_list),key=ballheight_list.count)\n",
    "        df_category.at[index, 'Winner'] = max(set(winner_list),key=winner_list.count)\n",
    "#         print(f\"{videoname}_{hitter}_hitframe_{hitframe}:{result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']}\")\n",
    "df_category.to_csv(f'test_submission_hitter_category_5fold.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8674b54",
   "metadata": {},
   "source": [
    "#  Output hitter+category+location submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "06686347",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973\n",
      "'00304mp4_Z_hitframe_459'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test_submission_hitter_category_5fold.csv')\n",
    "img_list = os.listdir('./Test_data/val_set_bigwidth')\n",
    "# result_dict = result_inference(df, category_model, regression_model, img_list, CFG['img_size'])\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        videoname = row['VideoName'].replace('.','')\n",
    "        hitframe = row['HitFrame']-1\n",
    "        hitter = row['Hitter']\n",
    "        df.at[index, 'HitterLocationX'] = result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']['hitterlocationX']\n",
    "        df.at[index, 'HitterLocationY'] = result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']['hitterlocationY']\n",
    "        df.at[index, 'DefenderLocationX'] = result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']['defenderlocationX']\n",
    "        df.at[index, 'DefenderLocationY'] = result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']['defenderlocationY']\n",
    "    except Exception as e:\n",
    "        print(index)\n",
    "        print(e)\n",
    "#     print(f\"{videoname}_{hitter}_hitframe_{hitframe}:{result_dict[f'{videoname}_{hitter}_hitframe_{hitframe}']}\")\n",
    "df.to_csv('test_submission_hitter_category_location.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
